{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clustering Assignment NLP - Fall 2016\n",
    "## Clustering Pride and Prejudice - does chapter clustering follow Darcy's proposals?\n",
    "### michelle.carney@berkeley.edu 20161118\n",
    "Initial clustering code is taken from brandonrose.org/clustering and past assignments from this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##All of the imports......\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import math\n",
    "import pprint\n",
    "import matplotlib\n",
    "from nltk import word_tokenize\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "from nltk.collocations import *\n",
    "import string, random\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get our text to look like Brandon Roses...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pptext = open(\"prideandprejudice.txt\",\"r\")\n",
    "rawbook = pptext.read()\n",
    "\n",
    "indices = [m.start() for m in re.finditer('Chapter', rawbook)]\n",
    "# print(indices)\n",
    "\n",
    "d = 'Chapter'\n",
    "start_index = 0\n",
    "chapters = []\n",
    "\n",
    "# print(rawbook[indices[0]])\n",
    "for index in indices:\n",
    "    chapters.append(rawbook[start_index:index])\n",
    "    start_index = index\n",
    "# print(chapters)\n",
    "\n",
    "titles = [] #titles is actually the chapter 'titles'\n",
    "synopses = [] #synopses is actually what is in the chapter\n",
    "for chapter in chapters:\n",
    "    titles.append(chapter[0:10].strip())\n",
    "    synopses.append(chapter[10::].strip())\n",
    "\n",
    "titles.pop(0)\n",
    "synopses.pop(0)\n",
    "# print(titles)\n",
    "# print(synopses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter 1', 'Chapter 2', 'Chapter 3', 'Chapter 4', 'Chapter 5', 'Chapter 6', 'Chapter 7', 'Chapter 8', 'Chapter 9', 'Chapter 10', 'Chapter 11', 'Chapter 12', 'Chapter 13', 'Chapter 14', 'Chapter 15', 'Chapter 16', 'Chapter 17', 'Chapter 18', 'Chapter 19', 'Chapter 20', 'Chapter 21', 'Chapter 22', 'Chapter 23', 'Chapter 24', 'Chapter 25', 'Chapter 26', 'Chapter 27', 'Chapter 28', 'Chapter 29', 'Chapter 30', 'Chapter 31', 'Chapter 32', 'Chapter 33', 'Chapter 34', 'Chapter 35', 'Chapter 36', 'Chapter 37', 'Chapter 38', 'Chapter 39', 'Chapter 40', 'Chapter 41', 'Chapter 42', 'Chapter 43', 'Chapter 44', 'Chapter 45', 'Chapter 46', 'Chapter 47', 'Chapter 48', 'Chapter 49', 'Chapter 50', 'Chapter 51', 'Chapter 52', 'Chapter 53', 'Chapter 54', 'Chapter 55', 'Chapter 56', 'Chapter 57', 'Chapter 58', 'Chapter 59', 'Chapter 60']\n"
     ]
    }
   ],
   "source": [
    "print(titles[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune, must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first ente\n"
     ]
    }
   ],
   "source": [
    "print(synopses[0][:200]) #first 200 characters in first synopses (for 'Chapter 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in synopses:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 121205 items in vocab_frame\n",
      "               words\n",
      "it                it\n",
      "is                is\n",
      "a                  a\n",
      "truth          truth\n",
      "univers  universally\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "\n",
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.11 s, sys: 42.3 ms, total: 4.15 s\n",
      "Wall time: 4.29 s\n",
      "(60, 746)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.22044605e-16   5.74433351e-01   6.46789546e-01 ...,   8.02659515e-01\n",
      "    6.45181669e-01   7.77070883e-01]\n",
      " [  5.74433351e-01   0.00000000e+00   5.83218362e-01 ...,   7.08211511e-01\n",
      "    6.27087153e-01   8.23754811e-01]\n",
      " [  6.46789546e-01   5.83218362e-01   0.00000000e+00 ...,   7.56108860e-01\n",
      "    6.52415916e-01   7.73313123e-01]\n",
      " ..., \n",
      " [  8.02659515e-01   7.08211511e-01   7.56108860e-01 ...,  -2.22044605e-16\n",
      "    6.00938213e-01   5.98233369e-01]\n",
      " [  6.45181669e-01   6.27087153e-01   6.52415916e-01 ...,   6.00938213e-01\n",
      "   -2.22044605e-16   6.48156471e-01]\n",
      " [  7.77070883e-01   8.23754811e-01   7.73313123e-01 ...,   5.98233369e-01\n",
      "    6.48156471e-01  -4.44089210e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Experimentation\n",
    "\n",
    "### My hypothesis is that chapters centered around the main plot point of proposal will be grouped together. In particular, it will be the 3 chunks of: Pre-Darcy proposal 1 (start - ch 34), post-Darcy proposal 1 and pre-Darcy proposal 2 (ch34 - 58), and post-darcy proposal 2 (ch58 - end).\n",
    "----------------------\n",
    "*Instructions:\n",
    "(a) Experiment with either k-means clustering or LDA on your adopted document collection to try to find topics in the collection.   Be sure to try a few different values of k.  (If you want to use some other variant of clustering, that is fine.)\n",
    "(b) Show your output in some easy-to-digest form.\n",
    "(c) Discuss how well it did or did not work.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss how well it worked:\n",
    "\n",
    "My hypothesis is that chapters centered around the main plot point of proposal will be grouped together. In particular, it will be the 3 chunks of: Pre-Darcy proposal 1 (start - ch 34), post-Darcy proposal 1 and pre-Darcy proposal 2 (ch34 - 58), and post-darcy proposal 2 (ch58 - end).\n",
    "\n",
    "If my hypothesis was correct, I would see the clusters for chapters 1-34, 34-58, and 58-60 when I do k=3. Instead, with k=3, I find there is a cluster that consists of chapters 1-18, a few chapters in the middle, and quite a few chapters at the end. When we look at this cluster in particular, we see that some of the words that determine this are ‘dance’ and ‘netherfield’ - which would make sense to the plot, initially the book takes place at a dance at netherfield, and in the end the plot also is around nether field and a dance. This lends more insight into the actual plot of the text - there is a circular motif about being ‘close to home’ or ‘home is where the heart is’ where Elizabeth and Darcy fall in love for the first time and get engaged.\n",
    "\n",
    "The second cluster in my hypothesis (k=3) is chapters around writing letters back and forth between family members, and my last cluster is chapters focused around family relationships and societal structures. These can also be seen as ‘motif’ or ’themed’ chapters: the second cluster motif being maintaining relationships over distance, and the last cluster being around the importance of family structure.\n",
    "\n",
    "While this does not match my hypothesis that chapters will cluster relative to the plot (mainly Darcy’s proposals), it is interesting to see how the motifs cluster the chapters. I expanded the number of words to 10 so I could see other words besides titles (mr., mrs.) and names, and these also support my claim of clustering around motifs. I also wanted to keep in titles (like mr. and mrs.) because Elizabeth’s parents are referred to as Mr. Bingley and Mrs. Bingley respectively, but I found they were deciding words in the clustering. \n",
    "\n",
    "I also tried to increase the cluster size, and I included my output from k=4 and k=10 as well. I found that k=4 did not support my “home is where the heart is” theme/motif claim - in fact, it did not cluster the beginning chapters and end chapters together like we saw in k=3. The k=4 clusters ended up clustering the chapters by people mentioned in the chapters. The k=10 clusters ended up clustering the chapters more by places or people in those chapters. I believe this is why the k=3 clustering is the most interesting of the clusters because it is more abstracted away from the people and places and kind of gets at overall theme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "## k = 3 (my hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 513 ms, sys: 5.57 ms, total: 518 ms\n",
      "Wall time: 537 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    22\n",
       "2    21\n",
       "0    17\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppbook = {'title': titles, 'synopsis': synopses, 'cluster': clusters}\n",
    "\n",
    "frame = pd.DataFrame(ppbook, index = [clusters] , columns = ['title', 'cluster'])\n",
    "frame['cluster'].value_counts() #number of chapters per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: b'collins', b'mr.', b'lady', b'catherine', b'charlotte', b'ladyship', b'rosings', b'daughters', b'cousin', b'sir',\n",
      "\n",
      "Cluster 0 titles: Chapter 13, Chapter 14, Chapter 15, Chapter 16, Chapter 19, Chapter 20, Chapter 22, Chapter 23, Chapter 28, Chapter 29, Chapter 30, Chapter 31, Chapter 32, Chapter 37, Chapter 38, Chapter 56, Chapter 57,\n",
      "\n",
      "Cluster 1 words: b'bingley', b'miss', b'mr.', b'mr.', b'dance', b'mrs.', b'dear', b'mother', b'netherfield', b'hurst',\n",
      "\n",
      "Cluster 1 titles: Chapter 1, Chapter 2, Chapter 3, Chapter 4, Chapter 5, Chapter 6, Chapter 7, Chapter 8, Chapter 9, Chapter 10, Chapter 11, Chapter 12, Chapter 17, Chapter 18, Chapter 21, Chapter 24, Chapter 33, Chapter 45, Chapter 53, Chapter 54, Chapter 55, Chapter 59,\n",
      "\n",
      "Cluster 2 words: b'wickham', b'gardiner', b'lydia', b'letter', b'mr.', b'mrs.', b'aunt', b'married', b'love', b'shall',\n",
      "\n",
      "Cluster 2 titles: Chapter 25, Chapter 26, Chapter 27, Chapter 34, Chapter 35, Chapter 36, Chapter 39, Chapter 40, Chapter 41, Chapter 42, Chapter 43, Chapter 44, Chapter 46, Chapter 47, Chapter 48, Chapter 49, Chapter 50, Chapter 51, Chapter 52, Chapter 58, Chapter 60,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :10]: #replace 10 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k = 4 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 475 ms, sys: 5.02 ms, total: 480 ms\n",
      "Wall time: 484 ms\n",
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: b'lydia', b'wickham', b'gardiner', b'forster', b'married', b'kitty', b'letter', b'uncle', b'lydia', b'mr.',\n",
      "\n",
      "Cluster 0 titles: Chapter 27, Chapter 39, Chapter 41, Chapter 42, Chapter 46, Chapter 47, Chapter 48, Chapter 49, Chapter 50, Chapter 51, Chapter 52,\n",
      "\n",
      "Cluster 1 words: b'bingley', b'mr.', b'miss', b'love', b'letter', b'walking', b'happy', b'wickham', b'netherfield', b'gardiner',\n",
      "\n",
      "Cluster 1 titles: Chapter 7, Chapter 8, Chapter 10, Chapter 11, Chapter 12, Chapter 21, Chapter 24, Chapter 25, Chapter 26, Chapter 32, Chapter 33, Chapter 34, Chapter 35, Chapter 36, Chapter 40, Chapter 43, Chapter 44, Chapter 45, Chapter 53, Chapter 54, Chapter 55, Chapter 58, Chapter 59, Chapter 60,\n",
      "\n",
      "Cluster 2 words: b'collins', b'mr.', b'lady', b'catherine', b'charlotte', b'ladyship', b'daughters', b'rosings', b'cousin', b'sir',\n",
      "\n",
      "Cluster 2 titles: Chapter 13, Chapter 14, Chapter 15, Chapter 16, Chapter 19, Chapter 20, Chapter 22, Chapter 23, Chapter 28, Chapter 29, Chapter 30, Chapter 31, Chapter 37, Chapter 38, Chapter 56, Chapter 57,\n",
      "\n",
      "Cluster 3 words: b'bingley', b'dance', b'mr.', b'ball', b'mr.', b'lucas', b'mr.', b'mrs.', b'girls', b'dear',\n",
      "\n",
      "Cluster 3 titles: Chapter 1, Chapter 2, Chapter 3, Chapter 4, Chapter 5, Chapter 6, Chapter 9, Chapter 17, Chapter 18,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "ppbook = {'title': titles, 'synopsis': synopses, 'cluster': clusters}\n",
    "\n",
    "frame = pd.DataFrame(ppbook, index = [clusters] , columns = ['title', 'cluster'])\n",
    "frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :10]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k = 10 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 739 ms, sys: 15.1 ms, total: 754 ms\n",
      "Wall time: 440 ms\n",
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: b'mr.', b'collins', b'bingley', b'suppose', b'colonel',\n",
      "\n",
      "Cluster 0 titles: Chapter 24, Chapter 32, Chapter 33,\n",
      "\n",
      "Cluster 1 words: b'bingley', b'mr.', b'mrs.', b'mother', b'mr.',\n",
      "\n",
      "Cluster 1 titles: Chapter 1, Chapter 2, Chapter 7, Chapter 9, Chapter 12, Chapter 21, Chapter 53, Chapter 54, Chapter 55,\n",
      "\n",
      "Cluster 2 words: b'collins', b'charlotte', b'mr.', b'lady', b'catherine',\n",
      "\n",
      "Cluster 2 titles: Chapter 28, Chapter 29, Chapter 30, Chapter 38,\n",
      "\n",
      "Cluster 3 words: b'dance', b'wickham', b'mr.', b'bingley', b'mr.',\n",
      "\n",
      "Cluster 3 titles: Chapter 3, Chapter 15, Chapter 16, Chapter 17, Chapter 18,\n",
      "\n",
      "Cluster 4 words: b'gardiner', b'mrs.', b'bingley', b'pemberley', b'miss',\n",
      "\n",
      "Cluster 4 titles: Chapter 25, Chapter 42, Chapter 43, Chapter 44, Chapter 45,\n",
      "\n",
      "Cluster 5 words: b'lydia', b'wickham', b'gardiner', b'letter', b'kitty',\n",
      "\n",
      "Cluster 5 titles: Chapter 26, Chapter 27, Chapter 39, Chapter 41, Chapter 46, Chapter 47, Chapter 48, Chapter 49, Chapter 50, Chapter 51, Chapter 52, Chapter 58, Chapter 59,\n",
      "\n",
      "Cluster 6 words: b'lady', b'catherine', b'ladyship', b'collins', b'mr.',\n",
      "\n",
      "Cluster 6 titles: Chapter 14, Chapter 19, Chapter 31, Chapter 37, Chapter 56, Chapter 57, Chapter 60,\n",
      "\n",
      "Cluster 7 words: b'wickham', b'mr.', b'mr.', b'letter', b'related',\n",
      "\n",
      "Cluster 7 titles: Chapter 34, Chapter 35, Chapter 36, Chapter 40,\n",
      "\n",
      "Cluster 8 words: b'collins', b'mr.', b'charlotte', b'lucas', b'mrs.',\n",
      "\n",
      "Cluster 8 titles: Chapter 13, Chapter 20, Chapter 22, Chapter 23,\n",
      "\n",
      "Cluster 9 words: b'bingley', b'miss', b'hurst', b'dance', b'mr.',\n",
      "\n",
      "Cluster 9 titles: Chapter 4, Chapter 5, Chapter 6, Chapter 8, Chapter 10, Chapter 11,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "ppbook = {'title': titles, 'synopsis': synopses, 'cluster': clusters}\n",
    "\n",
    "frame = pd.DataFrame(ppbook, index = [clusters] , columns = ['title', 'cluster'])\n",
    "frame['cluster'].value_counts() #number of films per cluster\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :5]: #replace 5 with n words per cluster -- For Word2Vec used 10 to find the adj and verb\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word2Vec\n",
    "\n",
    "\n",
    "### For Word2Vec, I had a hard time finding adjectives and verbs from my initial hypothesis clustering (k=3) -- so I used k=10 clustering printing 10 words for each cluster to determine some of these words (double checked with Andrea - removed the words down to 5 in order to save space on the notebook and make it not appeared cluttered). \n",
    "\n",
    "\n",
    "For the nouns, I looked at “letter,” “dance,” “ball,” “miss,” and “daughter.” What I found particularly interesting is when i used ‘dance (minus ‘move’)’ vs ‘ball (minus ‘round’)’ - what I found interesting is that word2vec had similar results (albeit ‘dance’ was more accurate, capturing words like ‘party’), but word net was entirely off, citing words like ‘musket ball.’ I also found that some of the words could easily be substituted from word2vec, like ‘ladies’ being substituted for ‘miss,’ while other words could not be subbed into Pride and Prejudice, like the word2vec ‘memo’ could never substitute for ‘letter’!\n",
    "\n",
    "For the verbs, I looked at ‘married,’ ‘dance,’ ‘read,’ ‘love,’ and ‘laugh.’ These were also very close, and could be substituted in a way that made sense in the text, but again word2vec overall did better at capturing similar words than word net (see ‘joke’ being substituted for ‘laugh’ - that would not work in PnP).\n",
    "\n",
    "Lastly, for the adjectives, I looked at ‘happy,’ ‘mean,’ ‘dear,’ ‘young,’ and ‘young (minus teenager)’. I had a harder time finding adjectives in my text from the clustering, even with k=10 and 10 words output, which is why I tried to compare young and young (minus teenager). Word2Vec for ‘young’ returns nouns, while I wanted adjectives, which is why i removed ‘teenager,’ and although it started returning adjectives (‘youngish,’ ‘talented,’ etc) it still could not be substituted. Wordnet only returned nouns for all of the adjectives except ‘happy.’ Interestingly enough, word2vec and word net returned ‘mean’ as like ‘means to an end’ - not an adjective, even after I set it on negative=imply (then it was more words like ‘heck’ and ‘gosh’).\n",
    "\n",
    "Overall, I think Jane Austen has a certain way of speaking that is not easily substitutable, but the best way to match her words (to get similar meanings) would be looking at nouns in word2vec.\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "*Instruction: Experiment with Word2Vec to find related terms for terms in your collection.  I recommend using the large pre-trained collection that is in the notebook we discussed in class.   You can do this one of two ways.  Either follow the instructions shown below, or come up with your own way to explore with it.*\n",
    "*(a) Select five nouns of interest from your collection, and compare what WordNet finds as the first 3 synsets to what Word2Vec finds as the top 5 rated similar nouns (using the most_similar() function).  State results are better for your collection in each case?  (you may use negative evidence if you like, by providing positive and negative example words).*\n",
    "*(b) Do the same for 5 adjectives.*\n",
    "*(c) Do the same for 5 verbs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.data import find\n",
    "#from nltk.download import word2vec_sample\n",
    "# word2vec_sampler=nltk.data.load('models/word2vec_sample/pruned.word2vec.txt')\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Letter:\n",
      "\t Word2Vec:\n",
      "[('memo', 0.7355137467384338), ('letters', 0.7233610153198242), ('missive', 0.6917874813079834), ('memorandum', 0.6289511919021606), ('statement', 0.6102292537689209)]\n",
      "\t Wordnet:\n",
      "[Synset('letter.n.01'), Synset('letter.n.02'), Synset('letter.n.03')]\n",
      "\n",
      " \n",
      " Dance:\n",
      "\t Word2Vec:\n",
      "[('dances', 0.5201530456542969), ('dancing', 0.5081790685653687), ('Party', 0.4721127450466156), ('parties', 0.4581678807735443), ('Dance', 0.4439452886581421)]\n",
      "\t Wordnet:\n",
      "[Synset('dance.n.01'), Synset('dance.n.02'), Synset('dancing.n.01')]\n",
      "\n",
      " \n",
      " Ball:\n",
      "\t Word2Vec:\n",
      "[('dancing', 0.561704158782959), ('dances', 0.5176332592964172), ('Dance', 0.49584853649139404), ('dancers', 0.47717174887657166), ('dancer', 0.47346895933151245)]\n",
      "\t Wordnet:\n",
      "[Synset('ball.n.01'), Synset('musket_ball.n.01'), Synset('ball.n.03')]\n",
      "\n",
      " \n",
      " Miss:\n",
      "\t Word2Vec:\n",
      "[('woman', 0.41856345534324646), ('gentleman', 0.39971935749053955), ('ladies', 0.38871461153030396), ('vivacious', 0.38015586137771606), ('hostess', 0.3671933710575104)]\n",
      "\t Wordnet:\n",
      "[Synset('girl.n.01'), Synset('miss.n.02'), Synset('miss.n.03')]\n",
      "\n",
      " \n",
      " Daughter:\n",
      "\t Word2Vec:\n",
      "[('mother', 0.8706232905387878), ('niece', 0.8637570142745972), ('granddaughter', 0.8516312837600708), ('son', 0.8468295335769653), ('daughters', 0.8136500716209412)]\n",
      "\t Wordnet:\n",
      "[Synset('daughter.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Letter:\")\n",
    "print('\\t Word2Vec:') \n",
    "print(model.most_similar(positive=['letter'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('letter')[:3])\n",
    "\n",
    "print(\"\\n \\n Dance:\")\n",
    "print('\\t Word2Vec:') \n",
    "print(model.most_similar(positive=['dance', 'party'], negative=['move'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('dance')[:3])\n",
    "\n",
    "print(\"\\n \\n Ball:\")\n",
    "print('\\t Word2Vec:') \n",
    "print(model.most_similar(positive=['ball', 'dance'], negative=['round'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('ball')[:3])\n",
    "\n",
    "print(\"\\n \\n Miss:\")\n",
    "print('\\t Word2Vec:') \n",
    "print(model.most_similar(positive=['miss', 'lady'], negative=['skip'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('miss')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n \\n Daughter:\")\n",
    "print('\\t Word2Vec:') \n",
    "print(model.most_similar(positive=['daughter'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('daughter')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Married:\n",
      "\t Word2Vec:\n",
      "[('divorced', 0.8280808925628662), ('wed', 0.7852946519851685), ('marry', 0.7635562419891357), ('remarried', 0.7542672753334045), ('marrying', 0.7138937711715698)]\n",
      "\t Wordnet:\n",
      "[Synset('married.n.01'), Synset('marry.v.01'), Synset('marry.v.02')]\n",
      "\n",
      " Dance:\n",
      "\t Word2Vec:\n",
      "[('dancing', 0.8380804061889648), ('dances', 0.8213123083114624), ('dancers', 0.7513905763626099), ('Dance', 0.7395395040512085), ('dancer', 0.6916880011558533)]\n",
      "\t Wordnet:\n",
      "[Synset('dance.n.01'), Synset('dance.n.02'), Synset('dancing.n.01')]\n",
      "\n",
      " Read:\n",
      "\t Word2Vec:\n",
      "[('reading', 0.698559045791626), ('reread', 0.6906844973564148), ('reads', 0.662841260433197), ('write', 0.6306530237197876), ('written', 0.5812059640884399)]\n",
      "\t Wordnet:\n",
      "[Synset('read.n.01'), Synset('read.v.01'), Synset('read.v.02')]\n",
      "\n",
      " Love:\n",
      "\t Word2Vec:\n",
      "[('loved', 0.6907792091369629), ('adore', 0.681687593460083), ('loves', 0.6618633270263672), ('passion', 0.6100708246231079), ('hate', 0.6003957986831665)]\n",
      "\t Wordnet:\n",
      "[Synset('love.n.01'), Synset('love.n.02'), Synset('beloved.n.01')]\n",
      "\n",
      " Laugh:\n",
      "\t Word2Vec:\n",
      "[('chuckle', 0.8091713190078735), ('laughing', 0.7357165813446045), ('giggle', 0.726399302482605), ('laughs', 0.7137472033500671), ('laughed', 0.6602218151092529)]\n",
      "\t Wordnet:\n",
      "[Synset('laugh.n.01'), Synset('laugh.n.02'), Synset('joke.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Married:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['married'], topn = 5)) #I have \n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('married')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Dance:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['dance'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('dance')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Read:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['read'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('read')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Love:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['love'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('love')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Laugh:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['laugh'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('laugh')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Happy:\n",
      "\t Word2Vec:\n",
      "[('glad', 0.7408891320228577), ('pleased', 0.6632169485092163), ('ecstatic', 0.6626911759376526), ('thrilled', 0.6514050364494324), ('satisfied', 0.6437950134277344)]\n",
      "\t Wordnet:\n",
      "[Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02')]\n",
      "\n",
      " Mean:\n",
      "\t Word2Vec:\n",
      "[('means', 0.6425307989120483), ('imply', 0.620326042175293), ('equate', 0.5891611576080322), ('necessarily', 0.572253406047821), ('necessitate', 0.569510817527771)]\n",
      "\t Wordnet:\n",
      "[Synset('mean.n.01'), Synset('mean.v.01'), Synset('entail.v.01')]\n",
      "\n",
      " Dear:\n",
      "\t Word2Vec:\n",
      "[('dearest', 0.6255167126655579), ('dearly', 0.563401997089386), ('beloved', 0.47946229577064514), ('cherished', 0.46813926100730896), ('cherish', 0.45106256008148193)]\n",
      "\t Wordnet:\n",
      "[Synset('beloved.n.01'), Synset('lamb.n.04'), Synset('beloved.s.01')]\n",
      "\n",
      " Young:\n",
      "\t Word2Vec:\n",
      "[('teenage', 0.6431924104690552), ('younger', 0.6350939869880676), ('youth', 0.5715783834457397), ('youngsters', 0.5662700533866882), ('teenagers', 0.5633072257041931)]\n",
      "\t Wordnet:\n",
      "[Synset('young.n.01'), Synset('young.n.02'), Synset('young.n.03')]\n",
      "\n",
      " Young (negative= 'teenager'):\n",
      "\t Word2Vec:\n",
      "[('younger', 0.42177993059158325), ('talented', 0.41552621126174927), ('youngish', 0.38448888063430786), ('seasoned', 0.3427583575248718), ('skilled', 0.3386733829975128)]\n",
      "\t Wordnet:\n",
      "[Synset('young.n.01'), Synset('young.n.02'), Synset('young.n.03')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Happy:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['happy'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('happy')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Mean:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['mean'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('mean')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Dear:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['dear'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('dear')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Young:\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['young'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('young')[:3])\n",
    "\n",
    "\n",
    "print(\"\\n Young (negative= 'teenager'):\")\n",
    "print('\\t Word2Vec:')\n",
    "print(model.most_similar(positive=['young'], negative=['teenager'], topn = 5))\n",
    "print('\\t Wordnet:') \n",
    "print(wn.synsets('young')[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
